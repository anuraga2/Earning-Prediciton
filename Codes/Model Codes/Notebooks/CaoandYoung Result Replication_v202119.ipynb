{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sklearn imports\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.pipeline import Pipeline, FeatureUnion\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# other imports\n",
    "import pyarrow.parquet as pq\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Reading the Parquet file and converting it to a pandas dataframe\n",
    "df = pq.read_table('CaoYouSample.parquet')\n",
    "df = df.to_pandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Data Cleaning steps before we jump into test - train split and transformations\n",
    "df_new = df.copy()\n",
    "rem_cols = ['CONM', 'TIC', 'CUSIP','FiscalYearEnd', 'FYEND_plus_3mos','LPERMNO','FYR','SIC','DATADATE']\n",
    "df_new.drop(rem_cols, axis = 1, inplace = True)\n",
    "df_new = df_new.drop_duplicates()\n",
    "df_new = df_new.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['GVKEY', 'FYEAR', 'ACT', 'AP', 'AT', 'CEQ', 'CHE', 'DLC', 'DLTT',\n",
       "       'INTAN', 'INVT', 'IVAO', 'LCT', 'LT', 'PPENT', 'RECT', 'TXP', 'COGS',\n",
       "       'DP', 'DVC', 'NOPIO', 'SALE', 'TXT', 'XAD', 'XIDO', 'XINT', 'XRD',\n",
       "       'XSGA', 'CSHO', 'SHRCD', 'EXCHCD', 'E', 'CFO', 'E_F1', 'SALE_D1',\n",
       "       'COGS_D1', 'XSGA_D1', 'XAD_D1', 'XRD_D1', 'DP_D1', 'XINT_D1',\n",
       "       'NOPIO_D1', 'TXT_D1', 'XIDO_D1', 'E_D1', 'DVC_D1', 'CHE_D1', 'INVT_D1',\n",
       "       'RECT_D1', 'ACT_D1', 'PPENT_D1', 'IVAO_D1', 'INTAN_D1', 'AT_D1',\n",
       "       'AP_D1', 'DLC_D1', 'TXP_D1', 'LCT_D1', 'DLTT_D1', 'LT_D1', 'CEQ_D1',\n",
       "       'CFO_D1'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_new.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Writing a class for the eda of the data frame\n",
    "class Panel_Data_Eda:\n",
    "    \n",
    "    ## Constructor of the class\n",
    "    def __init__(self,df):\n",
    "        self.df = df\n",
    "    \n",
    "    ## function to print the name of the data set\n",
    "    def print_shape(self):\n",
    "        print(\"The dimension of the data is: \", self.df.shape)\n",
    "        \n",
    "    \n",
    "    ## Function to bucket all the columns of the dataset into their datatypes\n",
    "    def col_classification(self):\n",
    "        emp_dict = {}\n",
    "        \n",
    "        ## Initializing type dictionary\n",
    "        for types in self.df.dtypes.unique():\n",
    "            if types in emp_dict:\n",
    "                continue\n",
    "            else:\n",
    "                emp_dict[types] = []\n",
    "        \n",
    "        # Appending column name to each data type \n",
    "        for col in self.df.columns:\n",
    "            emp_dict[self.df[col].dtypes].append(col)\n",
    "        \n",
    "        return emp_dict\n",
    "    \n",
    "    ## Function to do test train split of panel data\n",
    "    def test_train_splitter(self, cluster_col, time_col, window_width = 1):\n",
    "        \"\"\"\n",
    "        -- The test train splitter for the \n",
    "        \n",
    "        \"\"\"\n",
    "        gr1 = self.df.groupby([cluster_col], sort=False)\n",
    "        df2= gr1.apply(lambda x: x.sort_values([time_col], ascending=False))\n",
    "        df2 = df2.reset_index(drop=True)\n",
    "        test = df2.groupby(cluster_col).head(window_width)\n",
    "        jdf = pd.merge(df2, test, how='outer', indicator=True)\n",
    "        train = jdf.loc[jdf['_merge'] == 'left_only']\n",
    "        del train['_merge'], jdf\n",
    "        return train,test\n",
    "    \n",
    "    ## Function to plot Univariate Time Series Data  \n",
    "    def univariate_time_series_plot(self, name, xaxis, yaxis, cluster):\n",
    "\n",
    "        \"\"\"\n",
    "\n",
    "        -- This function takes in the object and spits out Univariate plots for the specified sets of columns\n",
    "        -- The parameter list is given below\n",
    "            * self: The object\n",
    "            * name: This variable contains the filteration criteria\n",
    "            * xaxis: The vairable that will make the x axis of the univariate time series plot (mostly time)\n",
    "            * yaxis: Variable that will make the y axis of the plot\n",
    "            * cluster: This is the column which contains clusters\n",
    "\n",
    "        \"\"\"\n",
    "\n",
    "        try:\n",
    "            samp_df = self.df.loc[self.df[cluster] == name,:]\n",
    "            if samp_df.shape[0] == 0:\n",
    "                print(\"No rows left in the data frame after filteration. The function will return None\")\n",
    "                return None\n",
    "            else:\n",
    "                title_text = \"Trend of \" + yaxis + \" for \" + name\n",
    "                fig = px.line(samp_df, x=xaxis, y=yaxis, title = title_text)\n",
    "                return fig\n",
    "\n",
    "        except ValueError:\n",
    "            print(\"The value x/y column does not exist in the data frame\")\n",
    "\n",
    "        except NameError:\n",
    "            print(\"The data frame that you are passing does not exist. Please review the input\")\n",
    "\n",
    "        # Except Clause \n",
    "        except KeyError:\n",
    "            print(\"The column containing the filteration criteria does not exist in the data frame\")\n",
    "    \n",
    "    \n",
    "    ## Function to plot multiple trend lines (For various metrics) in a single plot   \n",
    "    def multivariate_timeseries_plots(self, comp_name, col_lst, date_column, cluster):\n",
    "        \n",
    "        # Initializing the figure object\n",
    "        fig = go.Figure()\n",
    "        df = self.df.copy()\n",
    "        samp_df = df.loc[df[cluster] == comp_name]\n",
    "        \n",
    "        ## iterating through all the items of the column list\n",
    "        for item in col_lst:\n",
    "            fig.add_trace(go.Scatter(\n",
    "            x = samp_df[date_column],\n",
    "            y = samp_df[item],\n",
    "            mode = 'lines+markers',\n",
    "            name = item\n",
    "            ))\n",
    "        \n",
    "        # Adding the title\n",
    "        fig.update_layout(title = ','.join(col_lst)+' for '+comp_name)\n",
    "        \n",
    "        # returning the figure\n",
    "        return fig\n",
    "    \n",
    "    \n",
    "    # print the type of column\n",
    "    def column_type(self):\n",
    "        dict_type = {}\n",
    "        \n",
    "        for col in self.df.columns:\n",
    "            if self.df[col].dtypes not in dict_type.keys():\n",
    "                dict_type[self.df[col].dtypes] = 1\n",
    "            else:\n",
    "                dict_type[self.df[col].dtypes] += 1\n",
    "        \n",
    "        col_class = self.col_classification()\n",
    "        print('\\n**********************************\\n')\n",
    "        print(col_class)\n",
    "        print('\\n**********************************\\n')\n",
    "        print(dict_type)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Custom Transformer that extracts columns passed as argument to its constructor\n",
    "\n",
    "class FeatureSelector(BaseEstimator, TransformerMixin):\n",
    "    \n",
    "    # Class constructor\n",
    "    def __init__(self, feature_names):\n",
    "        print(\"Feature Constructor was called\")\n",
    "        self.feature_names = feature_names\n",
    "    \n",
    "    # Return self, nothing else to do here\n",
    "    def fit(self, X, y = None):\n",
    "        print(\"Feature Selector fit method was called\")\n",
    "        return self\n",
    "    \n",
    "    # Method that describes what we need this transformer to do\n",
    "    def transform(self, X, y = None):\n",
    "        print(\"Feature selector transform method was called\")\n",
    "        return X[self.feature_names]\n",
    "\n",
    "\n",
    "# Custom transformer for feature engineering\n",
    "class NumericalTransformer(BaseEstimator, TransformerMixin):\n",
    "    \n",
    "    def __init__(self):\n",
    "        print(\"Numerical Transformer Constructor called\")\n",
    "        return None\n",
    "    \n",
    "    def fit(self, X, y = None):\n",
    "        print(\"Numerical Transformer fit method was called\")\n",
    "        return self\n",
    "    \n",
    "    def transform(self, X, y= None):\n",
    "        \n",
    "        print(\"Numerical Transformer transform method was called\")\n",
    "        X_ = X.copy()\n",
    "        for col in X_.columns:\n",
    "            if col != 'CEQ':\n",
    "                col_str = col + '_PER_EQ'\n",
    "                # creating a new column\n",
    "                X_.loc[:,col_str] = X_[col] / X_['CEQ']\n",
    "                \n",
    "                # dropping the redundant column\n",
    "                X_.drop(col, axis = 1, inplace = True)\n",
    "            \n",
    "            else:\n",
    "                continue\n",
    "        \n",
    "        X_ = X_.replace([np.inf, -np.inf], np.nan)\n",
    "        X_ = X_.fillna(0)\n",
    "        return X_.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Making the EDA object\n",
    "obj2 = Panel_Data_Eda(df_new)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "dtype_dict = obj2.col_classification()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = [item for item in dtype_dict[list(dtype_dict.keys())[1]] if item != 'E_F1' and item != 'FYEAR']\n",
    "y = [item for item in dtype_dict[list(dtype_dict.keys())[1]] if item == 'E_F1']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "year = 2016\n",
    "\n",
    "train_X = df_new.loc[(df_new['FYEAR'] >= (year-10)) & (df_new['FYEAR'] <= (year-1)), X]\n",
    "train_y = df_new.loc[(df_new['FYEAR'] >= (year-10)) & (df_new['FYEAR'] <= (year-1)), y]\n",
    "\n",
    "test_X = df_new.loc[df_new['FYEAR'] == year, X]\n",
    "test_y = df_new.loc[df_new['FYEAR'] == year, y]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature Constructor was called\n",
      "Numerical Transformer Constructor called\n",
      "Feature Selector fit method was called\n",
      "Feature selector transform method was called\n",
      "Numerical Transformer fit method was called\n",
      "Numerical Transformer transform method was called\n"
     ]
    }
   ],
   "source": [
    "# Creating the transformation pipeline\n",
    "# Numerical Features to pass down the numerical pipeline\n",
    "numerical_features = X\n",
    "\n",
    "# Defining the steps in the numerical pipeline\n",
    "numerical_pipeline = Pipeline(steps = [\n",
    "    ('num_selector', FeatureSelector(numerical_features)),\n",
    "    ('num_transformer', NumericalTransformer())\n",
    "])\n",
    "\n",
    "\n",
    "# Preparing the data for the model\n",
    "X_prepared = numerical_pipeline.fit_transform(train_X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setting up the hyperparameter search space for the RF Model\n",
    "parameters = {'max_features':['auto'],'max_depth':[20,25,30,35],'min_samples_leaf':[15,20,25,50]}\n",
    "#parameters = {'max_features':['auto'],'max_depth':[20],'min_samples_leaf':[15]}\n",
    "\n",
    "# Setting up the Random forest model\n",
    "rf_mod = RandomForestRegressor(n_estimators=500,criterion='mse',oob_score=True,n_jobs=-1,random_state=10)\n",
    "\n",
    "# Setting up the Grid Search object\n",
    "grid_search = GridSearchCV(rf_mod, parameters, cv=5, n_jobs=-1, scoring='neg_mean_squared_error')\n",
    "\n",
    "# Fitting the model\n",
    "grid_search.fit(X_prepared, train_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature Selector fit method was called\n",
      "Feature selector transform method was called\n",
      "Numerical Transformer fit method was called\n",
      "Numerical Transformer transform method was called\n"
     ]
    }
   ],
   "source": [
    "test_X_prepared = numerical_pipeline.fit_transform(test_X)\n",
    "pred = grid_search.predict(test_X_prepared)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE: 26.315216182167962\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(f\"RMSE: {np.sqrt(mean_squared_error(test_y, pred))}\\n\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
